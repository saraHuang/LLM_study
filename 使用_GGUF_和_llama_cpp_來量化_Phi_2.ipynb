{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saraHuang/LLM_study/blob/main/%E4%BD%BF%E7%94%A8_GGUF_%E5%92%8C_llama_cpp_%E4%BE%86%E9%87%8F%E5%8C%96_Phi_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 本筆記本展示了如何使用 GGUF 和 llama.cpp 來量化 microsoft/phi-2。\n",
        "\n",
        "* `MODEL_ID`: `microsoft/phi-2`\n",
        "* `QUANTIZATION_METHOD`: 要使用的量化方法。\n",
        "- Q5_K_M：5位元，推薦，品質損失低。\n",
        "- Q4_K_M：4位元，推薦，提供平衡的品質。"
      ],
      "metadata": {
        "id": "8y_Rk94LzG7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 作者聯絡方式與社群媒體\n",
        "\n",
        "如果您有任何疑問或想要進一步交流， 也歡迎私訊聯絡我，或隨時關注我的社群媒體：\n",
        "\n",
        "* **GitHub**： [我的 GitHub 連結](https://github.com/Heng-xiu)  \n",
        "* **Hugging Face**： [我的 Hugging Face 連結](https://huggingface.co/Heng666)\n",
        "* **部落格**： [我的 Medium 連結](https://r23456999.medium.com/)\n",
        "\n",
        "感謝大家的支持，也希望透過這些管道與更多對生成式 AI、Agentic AI System  \n",
        "或其他技術領域感興趣的朋友們進行討論和交流！\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://ko-fi.com/hengshiousheu\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "z2IJxn-lQZgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 來開始量化模型"
      ],
      "metadata": {
        "id": "TNItN0MS7C5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 首先，來登入 HuggingFace\n",
        "\n",
        "由於我們將從 Hugging Face hub 下載基礎模型 `microsoft/phi-2`，並將我們量化過的模型上傳回 Hugging Face hub，所以讓我們先登入 Hugging Face。\n",
        "\n",
        "#### Google Colab 新功能\n",
        "我將我的 Hugging Face token 存儲在左側的秘密標籤中。將我的 token 儲存在這個秘密標籤的好處是，我不會在筆記本中暴露 token，且我可以將這個秘密配置應用於我所有的 Colab 筆記本。"
      ],
      "metadata": {
        "id": "2M5gVSa6sW73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsVSBVQ2XmAQ",
        "outputId": "6f8885d0-2caa-4ff7-db3d-1a851c7ca7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heng666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安裝環境"
      ],
      "metadata": {
        "id": "D743h_X-6uiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 安裝套件 llama.cpp\n",
        "\n",
        "以下示範中，需要安裝 llama.cpp 來協助我們量化基礎模型，開始之前先來安裝吧！"
      ],
      "metadata": {
        "id": "wprFbubXs84I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && cmake -B build && cmake --build build --config Release # optionally, add -DGGML_CUDA=ON to activate CUDA"
      ],
      "metadata": {
        "id": "JbSyITvEnhBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 下載，轉換，量化模型\n",
        "\n",
        "### 下載模型\n",
        "\n",
        "在這一系列操作中，我們將會先下載基礎模型, 接者轉會成為 FP16, 最後進行量化。\n"
      ],
      "metadata": {
        "id": "lt860NZztY4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "MODEL_ID = \"microsoft/phi-2\"\n",
        "QUANTIZATION_METHODS = [\"q4_k_m\", \"q3_k_m\"]\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "print(MODEL_NAME)\n",
        "\n",
        "# Download model\n",
        "!git lfs install\n",
        "\n",
        "!git clone https://huggingface.co/{MODEL_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD24jJxq7t3k",
        "outputId": "5a7cf488-d2e2-4ce6-cb1b-996c094c3e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "phi-2\n",
            "Git LFS initialized.\n",
            "Cloning into 'phi-2'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Total 127 (delta 0), reused 0 (delta 0), pack-reused 127 (from 1)\u001b[K\n",
            "Receiving objects: 100% (127/127), 1.15 MiB | 3.55 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "Filtering content: 100% (2/2), 1.17 GiB | 6.33 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00001-of-00002.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 進行推論\n",
        "\n",
        "現在我們已經量化好模型了，接著讓我們跑測試看看。"
      ],
      "metadata": {
        "id": "WqI1CPiXI4dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "print(\"Available models: \" + \", \".join(model_list))\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid name\")\n",
        "else:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNPL9WYg78l-",
        "outputId": "ea0150d1-3a80-4861-d043-02baf1ffaf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models: \n",
            "Enter your prompt: Hi there\n",
            "Name of the model (options: ): \n",
            "Invalid name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 上傳量化模型到 HuggingFace Hub 上頭\n",
        "\n",
        "水喔，接著上傳已經量化好的模型到 HuggingFaceHub 中吧"
      ],
      "metadata": {
        "id": "Ar8pO7bb80US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import create_repo , HfApi\n",
        "from google.colab import userdata\n",
        "\n",
        "username = username\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "api = HfApi(token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "# Create empty repo\n",
        "api.create_repo(\n",
        "    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")\n",
        "\n",
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=f\"*.gguf\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "UOyKfUD-8jmh",
        "outputId": "1ee7117e-46bc-4bba-8173-325d952bd295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Heng666/phi-2-GGUF/commit/06d9503c33f4a5ddf77d8b6bf60383d4ccf51478', commit_message='Upload folder using huggingface_hub', commit_description='', oid='06d9503c33f4a5ddf77d8b6bf60383d4ccf51478', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 撰寫良好的 README.md 文檔\n",
        "\n",
        "最後一步，撰寫良好的 README.md 文檔，可以觀察 Thebloke 所分享的格式後續進行，這邊我們列出擢為重要的內容，包含 language, tags, license。"
      ],
      "metadata": {
        "id": "BuEdgipKAPkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "license: apache-2.0\n",
        "pipeline_tag: text-generation\n",
        "tags:\n",
        "- finetuned\n",
        "inference: false\n",
        "base_model: microsoft/phi-2\n",
        "model_creator: Mocrosoft AI_\n",
        "model_name: microsoft/phi-2\n",
        "model_type: phi-2\n",
        "prompt_template: '<s>[INST] {prompt} [/INST]\n",
        "  '\n",
        "quantized_by: Heng666\n",
        "---\n",
        "# microsoft/phi-2 - GGUF\n",
        "\n",
        "This is a quantized model for `microsoft/phi-2`. Two quantization methods were used:\n",
        "- Q5_K_M: 5-bit, preserves most of the model's performance\n",
        "- Q4_K_M: 4-bit, smaller footprints, and saves more memory\n",
        "  \n",
        "<!-- description start -->\n",
        "## Description\n",
        "\n",
        "This repo contains GGUF format model files for [microsoft/phi-2](https://huggingface.co/microsoft/phi-2).\n",
        "\n",
        "This model was quantized in Google Colab."
      ],
      "metadata": {
        "id": "ySH2GLlXAX19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && cmake -B build && cmake --build build --config Release # optionally, add -DGGML_CUDA=ON to activate CUDA"
      ],
      "metadata": {
        "id": "mwK8fTV6Aro8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5dc7de8-e54a-4639-ff6d-429eb703242f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: llama.cpp: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b5787/llama-b5787-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7OwsXirnsYB",
        "outputId": "ccbcfcdb-02c6-4233-91ef-5f1d26dc7a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-01 06:15:06--  https://github.com/ggml-org/llama.cpp/releases/download/b5787/llama-b5787-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/d950c0b7-f80d-46f5-934a-4f9b63745b90?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250701%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250701T061506Z&X-Amz-Expires=1800&X-Amz-Signature=0012aa416f2f82b1556b4c92471fca9167a1a2040775e27c6dfe39ae0a10bf0e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b5787-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-01 06:15:06--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/d950c0b7-f80d-46f5-934a-4f9b63745b90?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250701%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250701T061506Z&X-Amz-Expires=1800&X-Amz-Signature=0012aa416f2f82b1556b4c92471fca9167a1a2040775e27c6dfe39ae0a10bf0e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b5787-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12951348 (12M) [application/octet-stream]\n",
            "Saving to: ‘llama-b5787-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b5787-bin-ubu 100%[===================>]  12.35M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-07-01 06:15:06 (92.3 MB/s) - ‘llama-b5787-bin-ubuntu-x64.zip’ saved [12951348/12951348]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 解壓縮\n",
        "!unzip llama-b5787-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE_-lpHLn5b6",
        "outputId": "04b0cf68-231b-4bad-b737-209e79a09e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b5787-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/LICENSE-curl  \n",
            "  inflating: build/bin/LICENSE-httplib  \n",
            "  inflating: build/bin/LICENSE-jsonhpp  \n",
            "  inflating: build/bin/LICENSE-linenoise  \n",
            "  inflating: build/bin/libggml-base.so  \n",
            "  inflating: build/bin/libggml-cpu-alderlake.so  \n",
            "  inflating: build/bin/libggml-cpu-haswell.so  \n",
            "  inflating: build/bin/libggml-cpu-icelake.so  \n",
            "  inflating: build/bin/libggml-cpu-sandybridge.so  \n",
            "  inflating: build/bin/libggml-cpu-sapphirerapids.so  \n",
            "  inflating: build/bin/libggml-cpu-skylakex.so  \n",
            "  inflating: build/bin/libggml-cpu-sse42.so  \n",
            "  inflating: build/bin/libggml-cpu-x64.so  \n",
            "  inflating: build/bin/libggml-rpc.so  \n",
            "  inflating: build/bin/libggml.so    \n",
            "  inflating: build/bin/libllama.so   \n",
            "  inflating: build/bin/libmtmd.so    \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-gemma3-cli  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-mtmd-cli  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/rpc-server    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd build/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL910DNdoGO7",
        "outputId": "94204d26-08c5-4874-c018-9c746c307904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/build/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL_ID = \"microsoft/phi-2\"\n",
        "QUANTIZATION_METHODS = [\"q4_k_m\", \"q3_k_m\"]"
      ],
      "metadata": {
        "id": "PS2mJ7T-m18B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsKYaoLvoY5Z",
        "outputId": "83eec10c-c48f-4cfa-aefc-e9825004f9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;32mlibggml-base.so\u001b[0m*                \u001b[01;32mlibmtmd.so\u001b[0m*           \u001b[01;32mllama-llava-cli\u001b[0m*\n",
            "\u001b[01;32mlibggml-cpu-alderlake.so\u001b[0m*       LICENSE               \u001b[01;32mllama-minicpmv-cli\u001b[0m*\n",
            "\u001b[01;32mlibggml-cpu-haswell.so\u001b[0m*         LICENSE-curl          \u001b[01;32mllama-mtmd-cli\u001b[0m*\n",
            "\u001b[01;32mlibggml-cpu-icelake.so\u001b[0m*         LICENSE-httplib       \u001b[01;32mllama-perplexity\u001b[0m*\n",
            "\u001b[01;32mlibggml-cpu-sandybridge.so\u001b[0m*     LICENSE-jsonhpp       \u001b[01;32mllama-quantize\u001b[0m*\n",
            "\u001b[01;32mlibggml-cpu-sapphirerapids.so\u001b[0m*  LICENSE-linenoise     \u001b[01;32mllama-qwen2vl-cli\u001b[0m*\n",
            "\u001b[01;32mlibggml-cpu-skylakex.so\u001b[0m*        \u001b[01;32mllama-batched-bench\u001b[0m*  \u001b[01;32mllama-run\u001b[0m*\n",
            "\u001b[01;32mlibggml-cpu-sse42.so\u001b[0m*           \u001b[01;32mllama-bench\u001b[0m*          \u001b[01;32mllama-server\u001b[0m*\n",
            "\u001b[01;32mlibggml-cpu-x64.so\u001b[0m*             \u001b[01;32mllama-cli\u001b[0m*            \u001b[01;32mllama-tokenize\u001b[0m*\n",
            "\u001b[01;32mlibggml-rpc.so\u001b[0m*                 \u001b[01;32mllama-gemma3-cli\u001b[0m*     \u001b[01;32mllama-tts\u001b[0m*\n",
            "\u001b[01;32mlibggml.so\u001b[0m*                     \u001b[01;32mllama-gguf-split\u001b[0m*     \u001b[01;32mrpc-server\u001b[0m*\n",
            "\u001b[01;32mlibllama.so\u001b[0m*                    \u001b[01;32mllama-imatrix\u001b[0m*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llama-cli -hf microsoft/phi-2:Q4_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "cPm5w7x8mhf_",
        "outputId": "538995ef-c8c3-497e-e41d-2d7d714b5127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-38-3229893042.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-38-3229893042.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ./llama-cli -hf microsoft/phi-2:Q4_0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OnpiLdyloJP5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}